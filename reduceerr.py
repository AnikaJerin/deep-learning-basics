# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ILI7ernkX12aL4ffneEmBXNnaJoblORx
"""

#variable allows us to a trainable parameters to our graph
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

w=tf.Variable([.3],tf.float32) #storing value .32
b=tf.Variable([-.3],tf.float32)

x=tf.placeholder(tf.float32)

##then we crate a linear model
model=w*x + b #actual output

##then we need to initialize all the variables in tensorflow program for that we must explicitly call a special operation which is tf.global...
init=tf.global_variables_initializer()
sess=tf.compat.v1.Session()
#sess.run(init)
#print(sess.run(model,{x:[1,2,3,4]}))
#in variables we create a model but we don't know how good is that. we are going to evaluate that here
y=tf.placeholder(tf.float32) #in the placeholder we'll be provide with the desired values and it'll give us the desired output y
sq_delta=tf.square(model-y) #we'll calculate the loss function(actual out.-desired ot)sq
loss=tf.reduce_sum(sq_delta) #then sum all sq_deltas and then we are going to define one single scalar as loss

#print(sess.run(loss,{x:[1,2,3,4],y:[0,-1,-2,-3]}))

#reducing error
#to reduce/minimize this error tensorflow provides optimizers that changes each 
#variables to minimize the loss function..simp;est optimizer is gradient descent
optimizer=tf.train.GradientDescentOptimizer(.01) #we have to call the function called tf. gradients(learning rate)
train=optimizer.minimize(loss)
sess.run(init)
for i in range(1000):
  sess.run(train,{x:[1,2,3,4],y:[0,-1,-2,-3]})
print(sess.run([w,b]))